{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliação do Conditional Random Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GwPAxAq4gSwv"
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "import os\n",
    "import random\n",
    "import functools\n",
    "import collections\n",
    "import joblib\n",
    "import random\n",
    "import nltk\n",
    "import sklearn\n",
    "import sklearn_crfsuite\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from nltk.corpus import PlaintextCorpusReader \n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "random.seed(1999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "MVGEkrevwLN4"
   },
   "outputs": [],
   "source": [
    "#função do tamanho da palavra (returna True se for maior que 4)\n",
    "def length(word):\n",
    "    if len(word) >= 4: \n",
    "        tamanho = True\n",
    "    else:\n",
    "        tamanho = False\n",
    "    return tamanho\n",
    "\n",
    "teste_tagger = joblib.load('POS_tagger_brill.pkl')\n",
    "\n",
    "def postag(word):\n",
    "    phrase = word\n",
    "    postag = teste_tagger.tag(word_tokenize(phrase))\n",
    "    return postag[0][1]\n",
    "\n",
    "#tamanho da setenca\n",
    "def tamsent(sent,i):\n",
    "    conta = []\n",
    "    valor = []\n",
    "    for i in range(len(sent)):\n",
    "        conta.append(sent[i].count(sent[i][0]))\n",
    "    valor = sum(conta)\n",
    "    return valor\n",
    "\n",
    "#frequencia da palavra na sentenca\n",
    "def freqwordsent(sent,word):\n",
    "    conta = []\n",
    "    valor = []\n",
    "    for j in range(len(sent)):\n",
    "        conta.append(sent[j].count(word))\n",
    "    valor = sum(conta)\n",
    "    return valor\n",
    "\n",
    "\n",
    "def word2features(sent, i):\n",
    "    word = sent[i][0]  \n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word': word,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word[-1:]': word[-1:],        \n",
    "        'word[:1]': word[:1],\n",
    "        'word[:2]': word[:2],\n",
    "        'word[:3]': word[:3],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag(word),\n",
    "        'postag[:2]': postag(word)[:1],\n",
    "        'postag[:2]': postag(word)[:2],\n",
    "        'tamanho': length(word),\n",
    "        'word.isalnum()' : word.isalnum(),\n",
    "        'len(word)': len(word),\n",
    "        'tamanho(sent)': tamsent(sent,i),\n",
    "        'freqwordsent' : freqwordsent(sent,word),   \n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        features.update({\n",
    "            '-1:word': word1,\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isdigit()': word1.isdigit(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag(word1),\n",
    "            '-1:postag[:2]': postag(word1)[:1],\n",
    "            '-1:postag[:2]': postag(word1)[:2],\n",
    "            '-1:word[-3:]': word1[-3:],\n",
    "            '-1:word[-2:]': word1[-2:],\n",
    "            '-1:word[-1:]': word1[-1:],\n",
    "            '-1:word[:1]': word1[:1],\n",
    "            '-1:word[:2]': word1[:2],\n",
    "            '-1:word[:3]': word1[:3],\n",
    "            '-1:len(word)': len(word1),\n",
    "            '-1:word.isalnum()' : word1.isalnum(),\n",
    "        })\n",
    "    else:\n",
    "        features['Inicio'] = True\n",
    "\n",
    "    if i > 1:\n",
    "        word2 = sent[i-2][0]\n",
    "        features.update({\n",
    "            '-2:word': word2,\n",
    "            '-2:word.lower()': word2.lower(),\n",
    "            '-2:word.istitle()': word2.istitle(),\n",
    "            '-2:word.isdigit()': word2.isdigit(),\n",
    "            '-2:word.isupper()': word2.isupper(),\n",
    "            '-2:postag': postag(word2),\n",
    "            '-2:postag[:2]': postag(word2)[:2],\n",
    "            '-2:word[-3:]': word2[-3:],\n",
    "            '-2:word[-2:]': word2[-2:],\n",
    "            '-2:word[-1:]': word2[-1:],\n",
    "            '-2:word[:1]': word2[:1],\n",
    "            '-2:word[:2]': word2[:2],\n",
    "            '-2:word[:3]': word2[:3],\n",
    "            '-2:len(word)': len(word2),\n",
    "            '-2:word.isalnum()' : word2.isalnum(),\n",
    "\n",
    "        })\n",
    "    if i < len(sent)-1:\n",
    "        word3 = sent[i+1][0]\n",
    "        features.update({\n",
    "            '+1:word': word3,\n",
    "            '+1:word.lower()': word3.lower(),\n",
    "            '+1:word.istitle()': word3.istitle(),\n",
    "            '+1:word.isdigit()': word3.isdigit(),\n",
    "            '+1:word.isupper()': word3.isupper(),\n",
    "            '+1:postag': postag(word3),\n",
    "            '+1:postag[:2]': postag(word3)[:2],\n",
    "            '+1:word[-3:]': word3[-3:],\n",
    "            '+1:word[-2:]': word3[-2:],\n",
    "            '+1:word[-1:]': word3[-1:],\n",
    "            '+1:word[:1]': word3[:1],\n",
    "            '+1:word[:2]': word3[:2],\n",
    "            '+1:word[:3]': word3[:3],\n",
    "            '+1:len(word)': len(word3),\n",
    "            '+1:word.isalnum()' : word3.isalnum()\n",
    "            })\n",
    "    else:\n",
    "        features['Final'] = True\n",
    "   \n",
    "    if i < len(sent)-2:\n",
    "        word4 = sent[i+2][0]\n",
    "        features.update({\n",
    "            '+2:word': word4,\n",
    "            '+2:word.lower()': word4.lower(),\n",
    "            '+2:word.istitle()': word4.istitle(),\n",
    "            '+2:word.isdigit()': word4.isdigit(),\n",
    "            '+2:word.isupper()': word4.isupper(),\n",
    "            '+2:postag': postag(word4),\n",
    "            '+2:postag[:2]': postag(word4)[:2],\n",
    "            '+2:word[-3:]': word4[-3:],\n",
    "            '+2:word[-2:]': word4[-2:],\n",
    "            '+2:word[-1:]': word4[-1:],\n",
    "            '+2:word[:1]': word4[:1],\n",
    "            '+2:word[:2]': word4[:2],\n",
    "            '+2:word[:3]': word4[:3],\n",
    "            '+2:len(word)': len(word4),\n",
    "            '+2:word.isalnum()' : word4.isalnum()\n",
    "        })     \n",
    "\n",
    "    return features\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, label in sent]\n",
    "\n",
    "lista = [\n",
    "        'bias',\n",
    "        'word',\n",
    "        'word.lower()',\n",
    "        'word[-3:]',\n",
    "        'word[-2:]',\n",
    "        'word[-1:]',\n",
    "        'word[:1]',\n",
    "        'word[:2]',\n",
    "        'word[:3]',\n",
    "        'word.isupper()',\n",
    "        'word.istitle()',\n",
    "        'word.isdigit()',\n",
    "        'postag','postag[:2]',\n",
    "        'postag[:2]',\n",
    "        'tamanho',\n",
    "        'word.isalnum()',\n",
    "        'len(word)',\n",
    "        'tamanho(sent)',\n",
    "        'freqwordsent',\n",
    "        '-1:word',\n",
    "        '-1:word.lower()',\n",
    "        '-1:word.istitle()',\n",
    "        '-1:word.isdigit()',\n",
    "        '-1:word.isupper()',\n",
    "        '-1:postag',\n",
    "        '-1:postag[:2]',\n",
    "        '-1:postag[:2]',\n",
    "        '-1:word[-3:]',\n",
    "        '-1:word[-2:]',\n",
    "        '-1:word[-1:]',\n",
    "        '-1:word[:1]',\n",
    "        '-1:word[:2]',\n",
    "        '-1:word[:3]',\n",
    "        '-1:len(word)',\n",
    "        '-1:word.isalnum()',\n",
    "        '-2:word',\n",
    "        '-2:word.lower()',\n",
    "        '-2:word.istitle()',\n",
    "        '-2:word.isdigit()',\n",
    "        '-2:word.isupper()',\n",
    "        '-2:postag',\n",
    "        '-2:postag[:2]',\n",
    "        '-2:word[-3:]',\n",
    "        '-2:word[-2:]',\n",
    "        '-2:word[-1:]',\n",
    "        '-2:word[:1]',\n",
    "        '-2:word[:2]',\n",
    "        '-2:word[:3]',\n",
    "        '-2:len(word)',\n",
    "        '-2:word.isalnum()',\n",
    "        '+1:word',\n",
    "        '+1:word.lower()',\n",
    "        '+1:word.istitle()',\n",
    "        '+1:word.isdigit()',\n",
    "        '+1:word.isupper()',\n",
    "        '+1:postag',\n",
    "        '+1:postag[:2]',\n",
    "        '+1:word[-3:]',\n",
    "        '+1:word[-2:]',\n",
    "        '+1:word[-1:]',\n",
    "        '+1:word[:1]',\n",
    "        '+1:word[:2]',\n",
    "        '+1:word[:3]',\n",
    "        '+1:len(word)',\n",
    "        '+1:word.isalnum()',\n",
    "        '+2:word',\n",
    "        '+2:word.lower()',\n",
    "        '+2:word.istitle()',\n",
    "        '+2:word.isdigit()',\n",
    "        '+2:word.isupper()',\n",
    "        '+2:postag',\n",
    "        '+2:postag[:2]',\n",
    "        '+2:word[-3:]',\n",
    "        '+2:word[-2:]',\n",
    "        '+2:word[-1:]',\n",
    "        '+2:word[:1]',\n",
    "        '+2:word[:2]',\n",
    "        '+2:word[:3]',\n",
    "        '+2:len(word)',\n",
    "        '+2:word.isalnum()'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "GDzwVxk6Zj93"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Número de atributos do modelo: 81'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"Número de atributos do modelo: {len(lista)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_PL = \"./dados-categorias/PLs/\"  \n",
    "DIR_ST = \"./dados-categorias/STs/\"\n",
    "DIR_C = \"./dados-categorias/Comentarios/\"\n",
    "# DIR_PL = \"./dados-tipos/PLs/\"  \n",
    "# DIR_ST = \"./dados-tipos/STs/\"\n",
    "# DIR_C = \"./dados-tipos/Comentarios/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files_pl = [DIR_PL+f for f in os.listdir(DIR_PL)]\n",
    "all_files_st = [DIR_ST+f for f in os.listdir(DIR_ST)]\n",
    "all_files_c = [DIR_C+f for f in os.listdir(DIR_C)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_conll_file(location:str)->list:\n",
    "    with open(location, \"r\") as f:\n",
    "        data = f.read()\n",
    "    data = data.split(\"\\n\\n\")\n",
    "    data = list(map(lambda x:x.split(\"\\n\"), data))\n",
    "    data.pop()\n",
    "    data = list(map(lambda x:[operator.itemgetter(*[0, -1])(y.split(\" \")) for y in x], data))\n",
    "    return data\n",
    "\n",
    "def combine_files(locations:list)->list:\n",
    "    extended = []\n",
    "    for f in locations:\n",
    "        extended.extend(process_conll_file(f))\n",
    "    return extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de Sentenças no Conjunto de Treinamento (PL): 6828\n",
      "Número de Sentenças no Conjunto de Teste (PL): 2698\n",
      "Número de Sentenças no Conjunto de Treinamento (ST): 592\n",
      "Número de Sentenças no Conjunto de Teste (ST): 198\n",
      "Número de Sentenças no Conjunto de Treinamento (Comentarios): 725\n",
      "Número de Sentenças no Conjunto de Teste (Comentarios): 242\n"
     ]
    }
   ],
   "source": [
    "# Divisão entre Conjuntos de Treinamento e de Teste\n",
    "# PL's\n",
    "train_size_pl = int(0.75*len(all_files_pl))\n",
    "random.shuffle(all_files_pl)\n",
    "train_files_pl = all_files_pl[:train_size_pl]\n",
    "test_files_pl = all_files_pl[train_size_pl:]\n",
    "    \n",
    "train_pl = combine_files(train_files_pl)\n",
    "test_pl = combine_files(test_files_pl)\n",
    "\n",
    "# ST's\n",
    "all_data_st = combine_files(all_files_st)\n",
    "random.shuffle(all_data_st)\n",
    "train_size_st = int(0.75*len(all_data_st))\n",
    "train_st = all_data_st[:train_size_st]\n",
    "test_st = all_data_st[train_size_st:]\n",
    "\n",
    "# Comentarios\n",
    "all_data_c = combine_files(all_files_c)\n",
    "random.shuffle(all_data_c)\n",
    "train_size_c = int(0.75*len(all_data_c))\n",
    "train_c = all_data_c[:train_size_c]\n",
    "test_c = all_data_c[train_size_c:]\n",
    "\n",
    "print(f\"Número de Sentenças no Conjunto de Treinamento (PL): {len(train_pl)}\")\n",
    "print(f\"Número de Sentenças no Conjunto de Teste (PL): {len(test_pl)}\")\n",
    "print(f\"Número de Sentenças no Conjunto de Treinamento (ST): {len(train_st)}\")\n",
    "print(f\"Número de Sentenças no Conjunto de Teste (ST): {len(test_st)}\")\n",
    "print(f\"Número de Sentenças no Conjunto de Treinamento (Comentarios): {len(train_c)}\")\n",
    "print(f\"Número de Sentenças no Conjunto de Teste (Comentarios): {len(test_c)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Validação cruzada com k=5\n",
    "# to_store = \"CRF-CV\"\n",
    "    \n",
    "# kfold = KFold(n_splits=5)\n",
    "# train = np.array(train_c, dtype=object)\n",
    "# i = 1\n",
    "# for t, tt in kfold.split(train):\n",
    "#     to_train = train[t].tolist()\n",
    "#     to_val = train[tt].tolist()\n",
    "    \n",
    "#     X_train = [sent2features(s) for s in to_train]\n",
    "#     y_train = [sent2labels(s) for s in to_train]\n",
    "\n",
    "#     X_test = [sent2features(s) for s in to_val]\n",
    "    \n",
    "#     crf = sklearn_crfsuite.CRF(\n",
    "#         algorithm='lbfgs',\n",
    "#         c1=0.9,\n",
    "#         c2=0.4,\n",
    "#         max_iterations=100,\n",
    "#         all_possible_transitions=True\n",
    "#     )\n",
    "#     crf.fit(X_train, y_train)\n",
    "    \n",
    "#     ycrf = crf.predict(X_test)\n",
    "    \n",
    "#     crf_file = \"\"\n",
    "#     for preds, true in zip(ycrf, to_val):\n",
    "#         for j in range(len(preds)):\n",
    "#             crf_file += true[j][0] + \" \" + true[j][1] + \" \" + preds[j] + \"\\n\"\n",
    "#         crf_file += \"\\n\"\n",
    "#     with open(f\"./{to_store}/predictions_file_{i}\", \"w\") as f:\n",
    "#         f.write(crf_file)\n",
    "#     print(i)\n",
    "#     i += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimento II: Transferência de Conhecimento entre diferentes corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extração Features PL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 33s, sys: 604 ms, total: 4min 34s\n",
      "Wall time: 4min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_pl = [sent2features(s) for s in train_pl]\n",
    "y_train_pl = [sent2labels(s) for s in train_pl]\n",
    "\n",
    "X_test_pl = [sent2features(s) for s in test_pl]\n",
    "y_test_pl = [sent2labels(s) for s in test_pl]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extração Features ST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 53s, sys: 544 ms, total: 2min 53s\n",
      "Wall time: 2min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_st = [sent2features(s) for s in train_st]\n",
    "y_train_st = [sent2labels(s) for s in train_st]\n",
    "\n",
    "X_test_st = [sent2features(s) for s in test_st]\n",
    "y_test_st = [sent2labels(s) for s in test_st]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extração Features Comentários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 18s, sys: 554 ms, total: 3min 18s\n",
      "Wall time: 3min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_c = [sent2features(s) for s in train_c]\n",
    "y_train_c = [sent2labels(s) for s in train_c]\n",
    "\n",
    "X_test_c = [sent2features(s) for s in test_c]\n",
    "y_test_c = [sent2labels(s) for s in test_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Treinamento PL\n",
    "crf_pl = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.9,\n",
    "    c2=0.4,\n",
    "    max_iterations=500,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf_pl.fit(X_train_pl, y_train_pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Treinamento ST\n",
    "crf_st = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.9,\n",
    "    c2=0.4,\n",
    "    max_iterations=500,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf_st.fit(X_train_st, y_train_st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 40s, sys: 60.2 ms, total: 3min 40s\n",
      "Wall time: 3min 40s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_transitions=True, c1=0.0, c2=0.0,\n",
       "    keep_tempfiles=None, max_iterations=500)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Treinamento Comentários\n",
    "crf_c = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.0,\n",
    "    c2=0.0,\n",
    "    max_iterations=500,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf_c.fit(X_train_c, y_train_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRF treinado nos Comentários\n",
    "y_crf_c_pl = crf_c.predict(X_test_pl)\n",
    "y_crf_c_st = crf_c.predict(X_test_st)\n",
    "y_crf_c_c = crf_c.predict(X_test_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_results(test_pl, y_crf_c_pl, \"CRF-C-PL\")\n",
    "store_results(test_st, y_crf_c_st, \"CRF-C-ST\")\n",
    "store_results(test_c, y_crf_c_c, \"CRF-C-C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Treinamento PL + ST\n",
    "X_train_plst = X_train_pl + X_train_st\n",
    "y_train_plst = y_train_pl + y_train_st\n",
    "\n",
    "crf_plst = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.9,\n",
    "    c2=0.4,\n",
    "    max_iterations=500,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf_plst.fit(X_train_plst, y_train_plst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Treinamento PL + Comentários\n",
    "X_train_plc = X_train_pl + X_train_c\n",
    "y_train_plc = y_train_pl + y_train_c\n",
    "\n",
    "crf_plc = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.9,\n",
    "    c2=0.4,\n",
    "    max_iterations=500,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf_plc.fit(X_train_plc, y_train_plc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Treinamento ST + Comentários\n",
    "X_train_stc = X_train_st + X_train_c\n",
    "y_train_stc = y_train_st + y_train_c\n",
    "\n",
    "crf_stc = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.9,\n",
    "    c2=0.4,\n",
    "    max_iterations=500,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf_stc.fit(X_train_stc, y_train_stc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Treinamento PL + ST + Comentários\n",
    "X_train_all = X_train_pl + X_train_st + X_train_c\n",
    "y_train_all = y_train_pl + y_train_st + y_train_c\n",
    "\n",
    "crf_all = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.9,\n",
    "    c2=0.4,\n",
    "    max_iterations=500,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf_all.fit(X_train_all, y_train_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRF treinado nos PL's\n",
    "y_crf_pl_pl = crf_pl.predict(X_test_pl)\n",
    "y_crf_pl_st = crf_pl.predict(X_test_st)\n",
    "y_crf_pl_c = crf_pl.predict(X_test_c)\n",
    "\n",
    "# CRF treinado nas ST's\n",
    "y_crf_st_pl = crf_st.predict(X_test_pl)\n",
    "y_crf_st_st = crf_st.predict(X_test_st)\n",
    "y_crf_st_c = crf_st.predict(X_test_c)\n",
    "\n",
    "# CRF treinado nos Comentários\n",
    "y_crf_c_pl = crf_c.predict(X_test_pl)\n",
    "y_crf_c_st = crf_c.predict(X_test_st)\n",
    "y_crf_c_c = crf_c.predict(X_test_c)\n",
    "\n",
    "# CRF treinado nos PL's + ST's\n",
    "y_crf_plst_pl = crf_plst.predict(X_test_pl)\n",
    "y_crf_plst_st = crf_plst.predict(X_test_st)\n",
    "y_crf_plst_c = crf_plst.predict(X_test_c)\n",
    "\n",
    "# CRF treinado nos PL's + Comentários\n",
    "y_crf_plc_pl = crf_plc.predict(X_test_pl)\n",
    "y_crf_plc_st = crf_plc.predict(X_test_st)\n",
    "y_crf_plc_c = crf_plc.predict(X_test_c)\n",
    "\n",
    "# CRF treinado nos ST's + Comentários\n",
    "y_crf_stc_pl = crf_stc.predict(X_test_pl)\n",
    "y_crf_stc_st = crf_stc.predict(X_test_st)\n",
    "y_crf_stc_c = crf_stc.predict(X_test_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_crf_all_pl = crf_all.predict(X_test_pl)\n",
    "y_crf_all_st = crf_all.predict(X_test_st)\n",
    "y_crf_all_c = crf_all.predict(X_test_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_results(test, y_pred, filename):\n",
    "    content = \"\"\n",
    "    for preds, true in zip(y_pred, test):\n",
    "        for j in range(len(preds)):\n",
    "            content += true[j][0] + \" \" + true[j][1] + \" \" + preds[j] + \"\\n\"\n",
    "        content += \"\\n\"\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_results(test_pl, y_crf_pl_pl, \"CRF-PL-PL\")\n",
    "store_results(test_pl, y_crf_st_pl, \"CRF-ST-PL\")\n",
    "store_results(test_pl, y_crf_c_pl, \"CRF-C-PL\")\n",
    "store_results(test_pl, y_crf_plst_pl, \"CRF-PLST-PL\")\n",
    "store_results(test_pl, y_crf_plc_pl, \"CRF-PLC-PL\")\n",
    "store_results(test_pl, y_crf_stc_pl, \"CRF-STC-PL\")\n",
    "\n",
    "store_results(test_st, y_crf_pl_st, \"CRF-PL-ST\")\n",
    "store_results(test_st, y_crf_st_st, \"CRF-ST-ST\")\n",
    "store_results(test_st, y_crf_c_st, \"CRF-C-ST\")\n",
    "store_results(test_st, y_crf_plst_st, \"CRF-PLST-ST\")\n",
    "store_results(test_st, y_crf_plc_st, \"CRF-PLC-ST\")\n",
    "store_results(test_st, y_crf_stc_st, \"CRF-STC-ST\")\n",
    "\n",
    "store_results(test_c, y_crf_pl_c, \"CRF-PL-C\")\n",
    "store_results(test_c, y_crf_st_c, \"CRF-ST-C\")\n",
    "store_results(test_c, y_crf_c_c, \"CRF-C-C\")\n",
    "store_results(test_c, y_crf_plst_c, \"CRF-PLST-C\")\n",
    "store_results(test_c, y_crf_plc_c, \"CRF-PLC-C\")\n",
    "store_results(test_c, y_crf_stc_c, \"CRF-STC-C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_results(test_pl, y_crf_all_pl, \"CRF-ALL-PL\")\n",
    "store_results(test_st, y_crf_all_st, \"CRF-ALL-ST\")\n",
    "store_results(test_c, y_crf_all_c, \"CRF-ALL-C\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "P_Ls9moj5oV0"
   ],
   "machine_shape": "hm",
   "name": "NER_Demanda2.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
