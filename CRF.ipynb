{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GwPAxAq4gSwv"
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "import os\n",
    "import random\n",
    "import functools\n",
    "import collections\n",
    "import joblib\n",
    "import random\n",
    "import nltk\n",
    "import sklearn\n",
    "import sklearn_crfsuite\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from nltk.corpus import PlaintextCorpusReader \n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "random.seed(1999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "MVGEkrevwLN4"
   },
   "outputs": [],
   "source": [
    "#função do tamanho da palavra (returna True se for maior que 4)\n",
    "def length(word):\n",
    "    if len(word) >= 4: \n",
    "        tamanho = True\n",
    "    else:\n",
    "        tamanho = False\n",
    "    return tamanho\n",
    "\n",
    "teste_tagger = joblib.load('POS_tagger_brill.pkl')\n",
    "\n",
    "def postag(word):\n",
    "    phrase = word\n",
    "    postag = teste_tagger.tag(word_tokenize(phrase))\n",
    "    return postag[0][1]\n",
    "\n",
    "#tamanho da setenca\n",
    "def tamsent(sent,i):\n",
    "    conta = []\n",
    "    valor = []\n",
    "    for i in range(len(sent)):\n",
    "        conta.append(sent[i].count(sent[i][0]))\n",
    "    valor = sum(conta)\n",
    "    return valor\n",
    "\n",
    "#frequencia da palavra na sentenca\n",
    "def freqwordsent(sent,word):\n",
    "    conta = []\n",
    "    valor = []\n",
    "    for j in range(len(sent)):\n",
    "        conta.append(sent[j].count(word))\n",
    "    valor = sum(conta)\n",
    "    return valor\n",
    "\n",
    "\n",
    "def word2features(sent, i):\n",
    "    word = sent[i][0]  \n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word': word,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word[-1:]': word[-1:],        \n",
    "        'word[:1]': word[:1],\n",
    "        'word[:2]': word[:2],\n",
    "        'word[:3]': word[:3],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag(word),\n",
    "        'postag[:2]': postag(word)[:1],\n",
    "        'postag[:2]': postag(word)[:2],\n",
    "        'tamanho': length(word),\n",
    "        'word.isalnum()' : word.isalnum(),\n",
    "        'len(word)': len(word),\n",
    "        'tamanho(sent)': tamsent(sent,i),\n",
    "        'freqwordsent' : freqwordsent(sent,word),   \n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        features.update({\n",
    "            '-1:word': word1,\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isdigit()': word1.isdigit(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag(word1),\n",
    "            '-1:postag[:2]': postag(word1)[:1],\n",
    "            '-1:postag[:2]': postag(word1)[:2],\n",
    "            '-1:word[-3:]': word1[-3:],\n",
    "            '-1:word[-2:]': word1[-2:],\n",
    "            '-1:word[-1:]': word1[-1:],\n",
    "            '-1:word[:1]': word1[:1],\n",
    "            '-1:word[:2]': word1[:2],\n",
    "            '-1:word[:3]': word1[:3],\n",
    "            '-1:len(word)': len(word1),\n",
    "            '-1:word.isalnum()' : word1.isalnum(),\n",
    "        })\n",
    "    else:\n",
    "        features['Inicio'] = True\n",
    "\n",
    "    if i > 1:\n",
    "        word2 = sent[i-2][0]\n",
    "        features.update({\n",
    "            '-2:word': word2,\n",
    "            '-2:word.lower()': word2.lower(),\n",
    "            '-2:word.istitle()': word2.istitle(),\n",
    "            '-2:word.isdigit()': word2.isdigit(),\n",
    "            '-2:word.isupper()': word2.isupper(),\n",
    "            '-2:postag': postag(word2),\n",
    "            '-2:postag[:2]': postag(word2)[:2],\n",
    "            '-2:word[-3:]': word2[-3:],\n",
    "            '-2:word[-2:]': word2[-2:],\n",
    "            '-2:word[-1:]': word2[-1:],\n",
    "            '-2:word[:1]': word2[:1],\n",
    "            '-2:word[:2]': word2[:2],\n",
    "            '-2:word[:3]': word2[:3],\n",
    "            '-2:len(word)': len(word2),\n",
    "            '-2:word.isalnum()' : word2.isalnum(),\n",
    "\n",
    "        })\n",
    "    if i < len(sent)-1:\n",
    "        word3 = sent[i+1][0]\n",
    "        features.update({\n",
    "            '+1:word': word3,\n",
    "            '+1:word.lower()': word3.lower(),\n",
    "            '+1:word.istitle()': word3.istitle(),\n",
    "            '+1:word.isdigit()': word3.isdigit(),\n",
    "            '+1:word.isupper()': word3.isupper(),\n",
    "            '+1:postag': postag(word3),\n",
    "            '+1:postag[:2]': postag(word3)[:2],\n",
    "            '+1:word[-3:]': word3[-3:],\n",
    "            '+1:word[-2:]': word3[-2:],\n",
    "            '+1:word[-1:]': word3[-1:],\n",
    "            '+1:word[:1]': word3[:1],\n",
    "            '+1:word[:2]': word3[:2],\n",
    "            '+1:word[:3]': word3[:3],\n",
    "            '+1:len(word)': len(word3),\n",
    "            '+1:word.isalnum()' : word3.isalnum()\n",
    "            })\n",
    "    else:\n",
    "        features['Final'] = True\n",
    "   \n",
    "    if i < len(sent)-2:\n",
    "        word4 = sent[i+2][0]\n",
    "        features.update({\n",
    "            '+2:word': word4,\n",
    "            '+2:word.lower()': word4.lower(),\n",
    "            '+2:word.istitle()': word4.istitle(),\n",
    "            '+2:word.isdigit()': word4.isdigit(),\n",
    "            '+2:word.isupper()': word4.isupper(),\n",
    "            '+2:postag': postag(word4),\n",
    "            '+2:postag[:2]': postag(word4)[:2],\n",
    "            '+2:word[-3:]': word4[-3:],\n",
    "            '+2:word[-2:]': word4[-2:],\n",
    "            '+2:word[-1:]': word4[-1:],\n",
    "            '+2:word[:1]': word4[:1],\n",
    "            '+2:word[:2]': word4[:2],\n",
    "            '+2:word[:3]': word4[:3],\n",
    "            '+2:len(word)': len(word4),\n",
    "            '+2:word.isalnum()' : word4.isalnum()\n",
    "        })     \n",
    "\n",
    "    return features\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, label in sent]\n",
    "\n",
    "lista = [\n",
    "        'bias',\n",
    "        'word',\n",
    "        'word.lower()',\n",
    "        'word[-3:]',\n",
    "        'word[-2:]',\n",
    "        'word[-1:]',\n",
    "        'word[:1]',\n",
    "        'word[:2]',\n",
    "        'word[:3]',\n",
    "        'word.isupper()',\n",
    "        'word.istitle()',\n",
    "        'word.isdigit()',\n",
    "        'postag','postag[:2]',\n",
    "        'postag[:2]',\n",
    "        'tamanho',\n",
    "        'word.isalnum()',\n",
    "        'len(word)',\n",
    "        'tamanho(sent)',\n",
    "        'freqwordsent',\n",
    "        '-1:word',\n",
    "        '-1:word.lower()',\n",
    "        '-1:word.istitle()',\n",
    "        '-1:word.isdigit()',\n",
    "        '-1:word.isupper()',\n",
    "        '-1:postag',\n",
    "        '-1:postag[:2]',\n",
    "        '-1:postag[:2]',\n",
    "        '-1:word[-3:]',\n",
    "        '-1:word[-2:]',\n",
    "        '-1:word[-1:]',\n",
    "        '-1:word[:1]',\n",
    "        '-1:word[:2]',\n",
    "        '-1:word[:3]',\n",
    "        '-1:len(word)',\n",
    "        '-1:word.isalnum()',\n",
    "        '-2:word',\n",
    "        '-2:word.lower()',\n",
    "        '-2:word.istitle()',\n",
    "        '-2:word.isdigit()',\n",
    "        '-2:word.isupper()',\n",
    "        '-2:postag',\n",
    "        '-2:postag[:2]',\n",
    "        '-2:word[-3:]',\n",
    "        '-2:word[-2:]',\n",
    "        '-2:word[-1:]',\n",
    "        '-2:word[:1]',\n",
    "        '-2:word[:2]',\n",
    "        '-2:word[:3]',\n",
    "        '-2:len(word)',\n",
    "        '-2:word.isalnum()',\n",
    "        '+1:word',\n",
    "        '+1:word.lower()',\n",
    "        '+1:word.istitle()',\n",
    "        '+1:word.isdigit()',\n",
    "        '+1:word.isupper()',\n",
    "        '+1:postag',\n",
    "        '+1:postag[:2]',\n",
    "        '+1:word[-3:]',\n",
    "        '+1:word[-2:]',\n",
    "        '+1:word[-1:]',\n",
    "        '+1:word[:1]',\n",
    "        '+1:word[:2]',\n",
    "        '+1:word[:3]',\n",
    "        '+1:len(word)',\n",
    "        '+1:word.isalnum()',\n",
    "        '+2:word',\n",
    "        '+2:word.lower()',\n",
    "        '+2:word.istitle()',\n",
    "        '+2:word.isdigit()',\n",
    "        '+2:word.isupper()',\n",
    "        '+2:postag',\n",
    "        '+2:postag[:2]',\n",
    "        '+2:word[-3:]',\n",
    "        '+2:word[-2:]',\n",
    "        '+2:word[-1:]',\n",
    "        '+2:word[:1]',\n",
    "        '+2:word[:2]',\n",
    "        '+2:word[:3]',\n",
    "        '+2:len(word)',\n",
    "        '+2:word.isalnum()'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "GDzwVxk6Zj93"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Número de atributos do modelo: 81'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"Número de atributos do modelo: {len(lista)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_PL = \"./dados-categorias/PLs/\"  \n",
    "DIR_ST = \"./dados-categorias/STs/\"\n",
    "# DIR_PL = \"./dados-tipos/PLs/\"  \n",
    "# DIR_ST = \"./dados-tipos/STs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files_pl = [DIR_PL+f for f in os.listdir(DIR_PL)]\n",
    "all_files_st = [DIR_ST+f for f in os.listdir(DIR_ST)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_conll_file(location:str)->list:\n",
    "    with open(location, \"r\") as f:\n",
    "        data = f.read()\n",
    "    data = data.split(\"\\n\\n\")\n",
    "    data = list(map(lambda x:x.split(\"\\n\"), data))\n",
    "    data.pop()\n",
    "    data = list(map(lambda x:[operator.itemgetter(*[0, -1])(y.split(\" \")) for y in x], data))\n",
    "    return data\n",
    "\n",
    "def combine_files(locations:list)->list:\n",
    "    extended = []\n",
    "    for f in locations:\n",
    "        extended.extend(process_conll_file(f))\n",
    "    return extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de Sentenças no Conjunto de Treinamento (PL): 7551\n",
      "Número de Sentenças no Conjunto de Teste (PL): 1975\n",
      "Número de Sentenças no Conjunto de Treinamento (ST): 592\n",
      "Número de Sentenças no Conjunto de Teste (ST): 198\n"
     ]
    }
   ],
   "source": [
    "# Divisão entre Conjuntos de Treinamento e de Teste\n",
    "# PL's\n",
    "train_size_pl = int(0.75*len(all_files_pl))\n",
    "random.shuffle(all_files_pl)\n",
    "train_files_pl = all_files_pl[:train_size_pl]\n",
    "test_files_pl = all_files_pl[train_size_pl:]\n",
    "    \n",
    "train_pl = combine_files(train_files_pl)\n",
    "test_pl = combine_files(test_files_pl)\n",
    "\n",
    "# ST's\n",
    "all_data_st = combine_files(all_files_st)\n",
    "random.shuffle(all_data_st)\n",
    "train_size_st = int(0.75*len(all_data_st))\n",
    "train_st = all_data_st[:train_size_st]\n",
    "test_st = all_data_st[train_size_st:]\n",
    "\n",
    "print(f\"Número de Sentenças no Conjunto de Treinamento (PL): {len(train_pl)}\")\n",
    "print(f\"Número de Sentenças no Conjunto de Teste (PL): {len(test_pl)}\")\n",
    "print(f\"Número de Sentenças no Conjunto de Treinamento (ST): {len(train_st)}\")\n",
    "print(f\"Número de Sentenças no Conjunto de Teste (ST): {len(test_st)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projetos de Lei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_store = \"CRF-CV\"\n",
    "    \n",
    "# kfold = KFold(n_splits=5)\n",
    "# train = np.array(train_pl, dtype=object)\n",
    "# i = 1\n",
    "# for t, tt in kfold.split(train):\n",
    "#     to_train = train[t].tolist()\n",
    "#     to_val = train[tt].tolist()\n",
    "    \n",
    "#     X_train = [sent2features(s) for s in to_train]\n",
    "#     y_train = [sent2labels(s) for s in to_train]\n",
    "\n",
    "#     X_test = [sent2features(s) for s in to_val]\n",
    "    \n",
    "#     crf = sklearn_crfsuite.CRF(\n",
    "#         algorithm='lbfgs',\n",
    "#         c1=0.9,\n",
    "#         c2=0.4,\n",
    "#         max_iterations=100,\n",
    "#         all_possible_transitions=True\n",
    "#     )\n",
    "#     crf.fit(X_train, y_train)\n",
    "    \n",
    "#     ycrf = crf.predict(X_test)\n",
    "    \n",
    "#     crf_file = \"\"\n",
    "#     for preds, true in zip(ycrf, to_val):\n",
    "#         for j in range(len(preds)):\n",
    "#             crf_file += true[j][0] + \" \" + true[j][1] + \" \" + preds[j] + \"\\n\"\n",
    "#         crf_file += \"\\n\"\n",
    "#     with open(f\"./{to_store}/predictions_file_{i}\", \"w\") as f:\n",
    "#         f.write(crf_file)\n",
    "#     i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "Y5QbgyoDiNOG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 55s, sys: 660 ms, total: 4min 55s\n",
      "Wall time: 4min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_pl = [sent2features(s) for s in train_pl]\n",
    "y_train_pl = [sent2labels(s) for s in train_pl]\n",
    "\n",
    "X_test_pl = [sent2features(s) for s in test_pl]\n",
    "y_test_pl = [sent2labels(s) for s in test_pl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "2GLCX7Qm49PG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 22s, sys: 359 ms, total: 1min 22s\n",
      "Wall time: 1min 23s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_transitions=True, c1=0.9, c2=0.4,\n",
       "    keep_tempfiles=None, max_iterations=100)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "crf_pl = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.9,\n",
    "    c2=0.4,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf_pl.fit(X_train_pl, y_train_pl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solicitações de Trabalho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 2s, sys: 1.54 s, total: 3min 3s\n",
      "Wall time: 3min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_st = [sent2features(s) for s in train_st]\n",
    "y_train_st = [sent2labels(s) for s in train_st]\n",
    "\n",
    "X_test_st = [sent2features(s) for s in test_st]\n",
    "y_test_st = [sent2labels(s) for s in test_st]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 57.4 s, sys: 120 ms, total: 57.5 s\n",
      "Wall time: 57.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_transitions=True, c1=0.9, c2=0.4,\n",
       "    keep_tempfiles=None, max_iterations=100)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "crf_st = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.9,\n",
    "    c2=0.4,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf_st.fit(X_train_st, y_train_st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projetos de Lei + Solicitações de Trabalho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 25s, sys: 284 ms, total: 2min 25s\n",
      "Wall time: 2min 25s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_transitions=True, c1=0.9, c2=0.4,\n",
       "    keep_tempfiles=None, max_iterations=100)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "X_train_plst = X_train_pl + X_train_st\n",
    "y_train_plst = y_train_pl + y_train_st\n",
    "\n",
    "X_test_plst = X_test_pl + X_test_st\n",
    "y_test_plst = y_test_pl + y_test_st\n",
    "\n",
    "crf_plst = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.9,\n",
    "    c2=0.4,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf_plst.fit(X_train_plst, y_train_plst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRF treinado nos projetos de lei\n",
    "y_crf_pl_pl = crf_pl.predict(X_test_pl)\n",
    "y_crf_pl_st = crf_pl.predict(X_test_st)\n",
    "y_crf_pl_plst = crf_pl.predict(X_test_plst)\n",
    "\n",
    "# CRF treinado nas solicitações de trabalho\n",
    "y_crf_st_pl = crf_st.predict(X_test_pl)\n",
    "y_crf_st_st = crf_st.predict(X_test_st)\n",
    "y_crf_st_plst = crf_st.predict(X_test_plst)\n",
    "\n",
    "# CRF treinado nos projetos de lei e solicitações de trabalho\n",
    "y_crf_plst_pl = crf_plst.predict(X_test_pl)\n",
    "y_crf_plst_st = crf_plst.predict(X_test_st)\n",
    "y_crf_plst_plst = crf_plst.predict(X_test_plst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_results(test, y_pred, filename):\n",
    "    content = \"\"\n",
    "    for preds, true in zip(y_pred, test):\n",
    "        for j in range(len(preds)):\n",
    "            content += true[j][0] + \" \" + true[j][1] + \" \" + preds[j] + \"\\n\"\n",
    "        content += \"\\n\"\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_plst = test_pl + test_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_results(test_pl, y_crf_pl_pl, \"CRF-PL-PL\")\n",
    "store_results(test_pl, y_crf_st_pl, \"CRF-ST-PL\")\n",
    "store_results(test_pl, y_crf_plst_pl, \"CRF-PLST-PL\")\n",
    "\n",
    "store_results(test_st, y_crf_pl_st, \"CRF-PL-ST\")\n",
    "store_results(test_st, y_crf_st_st, \"CRF-ST-ST\")\n",
    "store_results(test_st, y_crf_plst_st, \"CRF-PLST-ST\")\n",
    "\n",
    "store_results(test_plst, y_crf_pl_plst, \"CRF-PL-PLST\")\n",
    "store_results(test_plst, y_crf_st_plst, \"CRF-ST-PLST\")\n",
    "store_results(test_plst, y_crf_plst_plst, \"CRF-PLST-PLST\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "P_Ls9moj5oV0"
   ],
   "machine_shape": "hm",
   "name": "NER_Demanda2.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
