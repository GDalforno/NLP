{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Random Fields (CRF) for Sequence Tagging\n",
    "\n",
    "## Definition\n",
    "\n",
    "Let $\\mathcal{V}^m$ be the space of all senteces and $\\mathcal{S}^m$ the space of all possible states, where $|\\mathcal{S}|=k$. We will denote a sentence $\\vec{x} \\in \\mathcal{V}^m$ as $\\vec{x} = (x_1, \\dots, x_m)$ and a state $\\vec{s} \\in \\mathcal{S}^m$ as $\\vec{s} = (s_1, \\dots, s_m)$. We assume the existence of some feature vector $\\Phi:\\mathcal{V}^m \\times \\mathcal{S}^m \\rightarrow \\mathbb{R}^d$ and model the conditional probability $p(\\vec{s}|\\vec{x})$ as\n",
    "\n",
    "$$\n",
    "p(\\vec{s}|\\vec{x}, w) = \\frac{exp(w \\cdot \\Phi(\\vec{x}, \\vec{s}))}{\\sum_{\\vec{s}' \\in \\mathcal{S}^m} exp(w \\cdot \\Phi(\\vec{x}, \\vec{s}'))},\\;\\; w \\in \\mathbb{R}^d\n",
    "$$\n",
    "\n",
    "\n",
    "Those models that try to estimate the conditional probability $p(s|x)$ directly are called discriminative models. The CRF can also be seen as a single layer neural network with softmax as activation function or a variant of logistic regression.\n",
    "\n",
    "## Feature Vector\n",
    "\n",
    "One way to define the feature vector $\\Phi$ is to set it to $\\Phi(\\vec{x}, \\vec{s}) = \\sum_{i=1}^m \\phi(\\vec{x}, j, s_{j-1}, s_j)$. In this case, we are making the assumption that the state at time $t$ depends only on the state at time $t-1$, this makes the CRF a class of so called Maximum Entropy Markov Models (MEMM's), since it relies on the markovian independence assumption.\n",
    "\n",
    "## Parameter Estimation\n",
    "\n",
    "Assume we have a finite set of pairs $\\{(\\vec{x}_i, \\vec{s}_i)\\}_{i=1}^n \\subset \\mathcal{V}^m \\times \\mathcal{S}^m$, we set $w^*$ to be the vector that maximizes the regularized log-likehood function $L_{\\lambda}(w)$:\n",
    "\n",
    "$$\n",
    "w^* = \\arg\\max\\limits_{w \\in \\mathbb{R}^d} L_{\\lambda}(w) = \\arg\\max\\limits_{w \\in \\mathbb{R}^d} \\sum_{i=1}^n log(p(\\vec{s}_i, \\vec{x_i}, w)) - \\lambda\\frac{||w||^2}{2}\n",
    "$$\n",
    "\n",
    "Since $L_{\\lambda}$ is differentiable with respect to the parameter $w$, any gradient based optimization process can be applied here to retrieve $w^*$, the most common one for the CRF is the L-BFGS.\n",
    "\n",
    "## Decoding\n",
    "\n",
    "Once the model is trained, the predicted state for a given sentence $\\vec{x}$ is given by\n",
    "\n",
    "$$\n",
    "\\arg\\max\\limits_{\\vec{s}' \\in \\mathcal{S}^m} p(\\vec{s}'|\\vec{x}, w)\n",
    "$$\n",
    "\n",
    "Since $p$ is an monotone function, it is enough to output the state $\\vec{s}$ such that\n",
    "\n",
    "$$\n",
    "\\vec{s} = \\arg\\max\\limits_{\\vec{s}' \\in \\mathcal{S}^m} \\sum_{i=1}^m \\phi(\\vec{x}, j, s'_{j-1}, s'_j)\n",
    "$$\n",
    "\n",
    "We will now apply a variation of the Viterbi algorithm to compute $\\vec{s}$ efficiently, the procedure is the following:\n",
    "\n",
    "- Initialize $\\pi[1, s] = w \\cdot \\phi(\\vec{x}, 1, s_0, s)$, where $s_0$ is a special symbol;\n",
    "- For $j=2, \\dots, m; s=1, \\dots, k$\n",
    "$$\n",
    "\\pi[j, s] = \\max\\limits_{s' \\in \\mathcal{S}} \\pi[j-1, s'] + w \\cdot \\phi(\\vec{x}, j, s', s)\n",
    "$$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$\n",
    "\\max\\limits_{s_1, \\dots, s_m} \\sum_{i=1}^m w \\cdot \\phi(\\vec{x}, j, s_{j-1}, sj) = \\max\\limits_{s} \\pi[m, s]\n",
    "$$\n",
    "\n",
    "If one adds a pointer to track the indexes of the sequence of states that maximizes the probability $p$, we can easily recover the label in $\\mathcal{O}(mk^2)$\n",
    "\n",
    "## Drawbacks\n",
    "\n",
    "The feature vector $\\Phi$ must be defined a priori.\n",
    "\n",
    "## References\n",
    "\n",
    "All the ideas presented here are discussed in details in this link: http://www.cs.columbia.edu/~mcollins/crf.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import time\n",
    "from pprint import pprint\n",
    "from functools import reduce\n",
    "from operator import iconcat\n",
    "from nltk.tag import CRFTagger\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We will use the MIT restaurant corpus as a toy dataset to perform named entity recognition (NER), the data is available here: https://groups.csail.mit.edu/sls/downloads/restaurant/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(fname:str)->list:\n",
    "    with open(fname, \"r\") as f:\n",
    "        data = f.read()\n",
    "    data = data.split(\"\\n\\n\")\n",
    "    data = list(map(lambda x:x.split(\"\\n\"), data))\n",
    "    data = list(map(lambda x:[tuple(s.split(\"\\t\"))[::-1] for s in x], data))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = process_file(\"./datasets/restauranttrain.bio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('2', 'B-Rating'),\n",
       "  ('start', 'I-Rating'),\n",
       "  ('restaurants', 'O'),\n",
       "  ('with', 'O'),\n",
       "  ('inside', 'B-Amenity'),\n",
       "  ('dining', 'I-Amenity')],\n",
       " [('34', 'O')],\n",
       " [('5', 'B-Rating'),\n",
       "  ('star', 'I-Rating'),\n",
       "  ('resturants', 'O'),\n",
       "  ('in', 'B-Location'),\n",
       "  ('my', 'I-Location'),\n",
       "  ('town', 'I-Location')]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format nltk uses to represent tagged sentences is quite simple, the training set is a list of sentences. Each sentence is a list of tuples, the first element is a word and the second is its corresponding tag. Also note that, the tag O represents that the corresponding word is not an entity, and B-SOMETHING represents the entity beginning and I-SOMETHING represents both the intermediate and last words of the entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('',)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.pop() # Last sentence is empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are 7660 sentences in the training set'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"There are {len(train)} sentences in the training set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_list(data:list)->list:\n",
    "    return reduce(iconcat, data, [])\n",
    "\n",
    "def split_words_n_tags(data:list)->tuple:\n",
    "    words, tags = map(list, zip(*data))\n",
    "    return words, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pairs = to_list(train)\n",
    "all_words, all_tags = split_words_n_tags(all_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are 3804 unique words in the training set'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"There are {len(set(all_words))} unique words in the training set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are 17 unique tags in the training set'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"There are {len(set(all_tags))} unique tags in the training set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('O', 43670),\n",
       " ('B-Location', 3817),\n",
       " ('I-Location', 3658),\n",
       " ('B-Cuisine', 2839),\n",
       " ('I-Amenity', 2676),\n",
       " ('B-Amenity', 2541),\n",
       " ('B-Restaurant_Name', 1901),\n",
       " ('I-Restaurant_Name', 1668),\n",
       " ('B-Dish', 1475),\n",
       " ('I-Hours', 1283),\n",
       " ('B-Rating', 1070),\n",
       " ('B-Hours', 990),\n",
       " ('I-Dish', 767),\n",
       " ('B-Price', 730),\n",
       " ('I-Cuisine', 630),\n",
       " ('I-Rating', 527),\n",
       " ('I-Price', 283)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist = nltk.probability.FreqDist(all_tags)\n",
    "hist.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class nltk.probability.FreqDist builds a histogram that tells us how many times each tag appeared on the training set. We can already expect that the tags with low frequency will be the ones the model will have more trouble during testing time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Before training this model we need to setup the following paramaters:\n",
    "- The feature vector;\n",
    "- The paramaters for the L-BFGS optimizer.\n",
    "\n",
    "### Feature Vector\n",
    "\n",
    "The paramater $\\textit{feature_func}$ must be a function that takes two parameters: a list of tokens (list of strings) and an index (integer), and returns a list of features. The API already provides a default function, so we just need to modificate it to our purposes:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def _get_features(self, tokens, idx):\n",
    "        \"\"\"\n",
    "        Extract basic features about this word including \n",
    "             - Current Word \n",
    "             - Is Capitalized ?\n",
    "             - Has Punctuation ?\n",
    "             - Has Number ?\n",
    "             - Suffixes up to length 3\n",
    "        Note that : we might include feature over previous word, next word ect. \n",
    "        \n",
    "        :return : a list which contains the features\n",
    "        :rtype : list(str)    \n",
    "        \n",
    "        \"\"\" \n",
    "        token = tokens[idx]\n",
    "        \n",
    "        feature_list = []  \n",
    "        # Capitalization \n",
    "        if token[0].isupper():\n",
    "            feature_list.append('CAPITALIZATION')\n",
    "        \n",
    "        # Number \n",
    "        if re.search(self._pattern, token) is not None:\n",
    "            feature_list.append('HAS_NUM') \n",
    "        \n",
    "        # Punctuation\n",
    "        punc_cat = set([\"Pc\", \"Pd\", \"Ps\", \"Pe\", \"Pi\", \"Pf\", \"Po\"])\n",
    "        if all (unicodedata.category(x) in punc_cat for x in token):\n",
    "            feature_list.append('PUNCTUATION')\n",
    "        \n",
    "        # Suffix up to length 3\n",
    "        if len(token) > 1:\n",
    "            feature_list.append('SUF_' + token[-1:]) \n",
    "        if len(token) > 2: \n",
    "            feature_list.append('SUF_' + token[-2:])    \n",
    "        if len(token) > 3: \n",
    "            feature_list.append('SUF_' + token[-3:])\n",
    "            \n",
    "        feature_list.append('WORD_' + token )\n",
    "        \n",
    "        return feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_func(tokens, idx):\n",
    "    token = tokens[idx]\n",
    "    \n",
    "    feature_list = []\n",
    "    \n",
    "    # We have hours, rating, price, then the presence of digits could be a useful feature\n",
    "    if not token.isalpha():\n",
    "        feature_list.append(\"HAS_NUMBER\")\n",
    "    \n",
    "    n = len(token)\n",
    "    # Let's extract some prefixes and suffixes from the current word\n",
    "    if n>1:\n",
    "        feature_list.append(\"PREF_\"+token[:2])\n",
    "        feature_list.append(\"SUF_\"+token[-1:])\n",
    "    if n>2:\n",
    "        feature_list.append(\"PREF_\"+token[:3])\n",
    "        feature_list.append(\"SUF_\"+token[-2:])\n",
    "        \n",
    "    if n>3:\n",
    "        feature_list.append(\"PREF_\"+token[:4])\n",
    "        feature_list.append(\"SUF_\"+token[-3:])\n",
    "        \n",
    "    if idx==0:\n",
    "        return feature_list\n",
    "        \n",
    "    # The same for the previous word\n",
    "    previous_token = tokens[idx-1]\n",
    "    \n",
    "    if not previous_token.isalpha():\n",
    "        feature_list.append(\"PHAS_NUMBER\")\n",
    "    \n",
    "    n = len(previous_token)\n",
    "    if n>1:\n",
    "        feature_list.append(\"PPREF_\"+previous_token[:2])\n",
    "        feature_list.append(\"PSUF_\"+previous_token[-1:])\n",
    "    if n>2:\n",
    "        feature_list.append(\"PPREF_\"+previous_token[:3])\n",
    "        feature_list.append(\"PSUF_\"+previous_token[-2:])\n",
    "        \n",
    "    if n>3:\n",
    "        feature_list.append(\"PPREF_\"+previous_token[:4])\n",
    "        feature_list.append(\"PSUF_\"+previous_token[-3:])\n",
    "        \n",
    "    if idx==len(tokens)-1:\n",
    "        return feature_list\n",
    "    \n",
    "    # The same for the next word\n",
    "    next_token = tokens[idx+1]\n",
    "    \n",
    "    if not next_token.isalpha():\n",
    "        feature_list.append(\"NHAS_NUMBER\")\n",
    "    \n",
    "    n = len(next_token)\n",
    "    if n>1:\n",
    "        feature_list.append(\"NPREF_\"+next_token[:2])\n",
    "        feature_list.append(\"NSUF_\"+next_token[-1:])\n",
    "    if n>2:\n",
    "        feature_list.append(\"NPREF_\"+next_token[:3])\n",
    "        feature_list.append(\"NSUF_\"+next_token[-2:])\n",
    "        \n",
    "    if n>3:\n",
    "        feature_list.append(\"NPREF_\"+next_token[:4])\n",
    "        feature_list.append(\"NSUF_\"+next_token[-3:])\n",
    "        \n",
    "    return feature_list    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the output of this function looks like for some sentence in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2', 'start', 'restaurants', 'with', 'inside', 'dining']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy = [w for w,t in train[0]]\n",
    "toy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HAS_NUMBER']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_func(toy, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PREF_st',\n",
       " 'SUF_t',\n",
       " 'PREF_sta',\n",
       " 'SUF_rt',\n",
       " 'PREF_star',\n",
       " 'SUF_art',\n",
       " 'PHAS_NUMBER',\n",
       " 'NPREF_re',\n",
       " 'NSUF_s',\n",
       " 'NPREF_res',\n",
       " 'NSUF_ts',\n",
       " 'NPREF_rest',\n",
       " 'NSUF_nts']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_func(toy, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PREF_di',\n",
       " 'SUF_g',\n",
       " 'PREF_din',\n",
       " 'SUF_ng',\n",
       " 'PREF_dini',\n",
       " 'SUF_ing',\n",
       " 'PPREF_in',\n",
       " 'PSUF_e',\n",
       " 'PPREF_ins',\n",
       " 'PSUF_de',\n",
       " 'PPREF_insi',\n",
       " 'PSUF_ide']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_func(toy, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the feature space will be huge, fortunately the log-linear models are well prepared to deal with sparsity. The next step is to define the parameters for the optimizer L-BFGS\n",
    "\n",
    "### L-BFGS parameters\n",
    "\n",
    "The possible (actually the most important) parameters to set are:\n",
    "- feature.possible_states : Force to generate possible state features;\n",
    "- c1: Coefficient for L1 regularization;\n",
    "- c2: Coefficient for L2 regularization;\n",
    "- max_iterations: The maximum number of iterations for L-BFGS optimization;\n",
    "- num_memories: The number of limited memories for approximating the inverse hessian matrix;\n",
    "- period: The duration of iterations to test the stopping criterion;\n",
    "\n",
    "The last 3 are useful when the dataset is too large, they control for how long the model will keep training and how much memory it is allowed to store. The first 3 are related to the optimization process itself, L2 regularization to help out with generalization, L1 to perform feature selection and feature.possible_states to add more features to the model related to the states themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRFTaggerCustom(CRFTagger):\n",
    "    def probability(self, tokens):\n",
    "        y = self.tag(tokens)\n",
    "        y = list(list(zip(*y))[1])\n",
    "        return self._tagger.probability(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\textit{nltk}$ package does not provide a method to calculate the probability $P(\\vec{s}|\\vec{x})$ so we need to add a custom method to the original API as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf = CRFTaggerCustom(feature_func, training_opt={\"feature.possible_states\":True, \"c1\":0.25, \"c2\":0.3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CRF took 93.97305 seconds to train'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tic = time.time()\n",
    "crf.train(train, \"model.cfr.tagger\")  # The trained model will be save to a file by default\n",
    "toc = time.time()\n",
    "f\"CRF took {(toc-tic):.5f} seconds to train\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick a sentence in the test set to see what we can get from the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('',)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = process_file(\"./datasets/restauranttest.bio\")\n",
    "test.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['any', 'bbq', 'places', 'open', 'before', '5', 'nearby']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy = [w for w,t in test[2]]\n",
    "toy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.613394375287388"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf.probability(toy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said before, this is the probability of the label $\\vec{s}$ appears given the sentence $\\vec{x}$. It can also be seen as an uncertainty measure, in the sense that, if the probability is high, the model is more confident about the prediction. Although, this does not mean that the prediction will be correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('any', 'O'),\n",
       " ('bbq', 'B-Cuisine'),\n",
       " ('places', 'O'),\n",
       " ('open', 'B-Hours'),\n",
       " ('before', 'I-Hours'),\n",
       " ('5', 'I-Hours'),\n",
       " ('nearby', 'B-Location')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf.tag(toy)  # Tags the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('any', 'O'),\n",
       " ('bbq', 'B-Cuisine'),\n",
       " ('places', 'O'),\n",
       " ('open', 'B-Hours'),\n",
       " ('before', 'I-Hours'),\n",
       " ('5', 'I-Hours'),\n",
       " ('nearby', 'B-Location')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[2]  # Compare with the actual labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's test our model on all sentences in the testing set to see how it performed. Firstly, we will unlabel the testing set to simulate when real predictions are desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrive_sents(data:list)->list:\n",
    "    return list(map(lambda x:[w for w,t in x], data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, labels = split_words_n_tags(to_list(test))\n",
    "unlabeled_sents = retrive_sents(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a', 'four', 'star', 'restaurant', 'with', 'a', 'bar'],\n",
       " ['any', 'asian', 'cuisine', 'around'],\n",
       " ['any', 'bbq', 'places', 'open', 'before', '5', 'nearby']]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_sents[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By calling the method $\\textit{tag_sents}$ we can tag all sentences at once using the CRF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CRF took 0.17402 seconds to tag all sequences in the testing set'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tic = time.time()\n",
    "preds = crf.tag_sents(unlabeled_sents)\n",
    "toc = time.time()\n",
    "f\"CRF took {(toc-tic):.5f} seconds to tag all sequences in the testing set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-Rating',\n",
       " 'I-Rating',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-Amenity',\n",
       " 'O',\n",
       " 'B-Cuisine',\n",
       " 'O']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, preds = split_words_n_tags(to_list(preds))\n",
    "preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-Rating',\n",
       " 'I-Rating',\n",
       " 'O',\n",
       " 'B-Location',\n",
       " 'I-Location',\n",
       " 'B-Amenity',\n",
       " 'O',\n",
       " 'B-Cuisine',\n",
       " 'O']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a list with all predicted tags regardless the sentence, since we are not interested in sentence-wise performance. The $\\textit{labels}$ variable is a list paired with the $\\textit{preds}$, so we can call the $\\textit{classification_report}$ function from the $\\textit{sklearn}$ API to measure the accuracy of the CRF's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('                   precision    recall  f1-score   support\\n'\n",
      " '\\n'\n",
      " '        B-Amenity       0.77      0.70      0.73       533\\n'\n",
      " '        B-Cuisine       0.86      0.85      0.86       532\\n'\n",
      " '           B-Dish       0.77      0.80      0.78       288\\n'\n",
      " '          B-Hours       0.74      0.68      0.71       212\\n'\n",
      " '       B-Location       0.86      0.84      0.85       812\\n'\n",
      " '          B-Price       0.82      0.79      0.81       171\\n'\n",
      " '         B-Rating       0.80      0.82      0.81       201\\n'\n",
      " 'B-Restaurant_Name       0.88      0.79      0.83       402\\n'\n",
      " '        I-Amenity       0.72      0.72      0.72       524\\n'\n",
      " '        I-Cuisine       0.66      0.65      0.66       135\\n'\n",
      " '           I-Dish       0.70      0.76      0.73       121\\n'\n",
      " '          I-Hours       0.86      0.86      0.86       295\\n'\n",
      " '       I-Location       0.83      0.84      0.84       788\\n'\n",
      " '          I-Price       0.79      0.64      0.71        66\\n'\n",
      " '         I-Rating       0.88      0.67      0.76       125\\n'\n",
      " 'I-Restaurant_Name       0.80      0.72      0.76       392\\n'\n",
      " '                O       0.93      0.95      0.94      8659\\n'\n",
      " '\\n'\n",
      " '         accuracy                           0.89     14256\\n'\n",
      " '        macro avg       0.81      0.77      0.79     14256\\n'\n",
      " '     weighted avg       0.89      0.89      0.89     14256\\n')\n"
     ]
    }
   ],
   "source": [
    "pprint(metrics.classification_report(labels, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the model performed quite well in this dataset, but again to make sure it is practically useful it is necessary to dig deeper into the analysis of those metrics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
