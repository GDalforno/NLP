{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Markov Models (HMM's) for Sequence Tagging\n",
    "\n",
    "## Generative Models\n",
    "Let $\\mathcal{V}$ be a vocabulary and $\\mathcal{K}$ a set of tags, define \n",
    "\n",
    "$$\n",
    "S = \\{(x_1, \\dots, x_n, y_1, \\dots, y_n):x_i \\in \\mathcal{V}, y_i \\in \\mathcal{K}\\}\n",
    "$$\n",
    "\n",
    "A generative model is a function $p$ satisfying\n",
    "\n",
    "- $p(x_1, \\dots, x_n, y_1, \\dots, y_n)>0, \\forall (x_1, \\dots, x_n, y_1, \\dots, y_n) \\in S$\n",
    "- $\\sum \\limits_{(x_1, \\dots, x_n, y_1, \\dots, y_n) \\in S} p(x_1, \\dots, x_n, y_1, \\dots, y_n) = 1$\n",
    "\n",
    "Within these conditions, we define the tagger $f:\\mathcal{V}^n \\rightarrow \\mathcal{K}^n$ as\n",
    "\n",
    "$$\n",
    "f(x_1, x_2, \\dots, x_n) = \\arg\\,\\max\\limits_{y_1, \\dots, y_n} p(x_1, \\dots, x_n, y_1, \\dots, y_n)\n",
    "$$\n",
    "\n",
    "## Trigram HMM's\n",
    "\n",
    "### Definition\n",
    "\n",
    "A Trigram HMM's is a finite vocabulary $\\mathcal{V}$, a finite set of tags $\\mathcal{K}$ along with the following parameters:\n",
    "\n",
    "- $q(u| v, s)$, where $u, v, s \\in \\mathcal{K}$. This can be thought as the probability of seeing the tag $u$ immediately after seeing the tags $v$ and $s$;\n",
    "- $e(x| s)$, where $x \\in \\mathcal{V}$ and $s \\in \\mathcal{K}$. This can be thought as the probability of seeing the word $x$ paired with the tag $s$.\n",
    "\n",
    "By taking the set $S$ as before we define\n",
    "\n",
    "$$\n",
    "p(x_1, \\dots, x_n, y_1, \\dots, y_n) = \\prod_{i=1}^{n+1}q(y_i|y_{i-1},y_{i-2})\\prod_{i=1}^n e(x_i|y_i)\n",
    "$$\n",
    "\n",
    "Note that, we are assuming our sentences are second order Markov sentences, i.e, the state at time $t$ depends only on the states at time $t-1$ and $t-2$. That's the reason why the probability function $p$ is reduced to the form above.\n",
    "\n",
    "### Parameter Estimation\n",
    "\n",
    "Define $c(u, v, s)$ to be the number of times the sequence of three states $(u,v,s)$ appears on the training set, similarly $c(u, v)$ is the number fo times the pair $(u,v)$ appears and finally, $c(u)$ counts the number of times the state $s$ shows up. Moreover, define $c(s \\leadsto x)$ to be the number of times the word $x$ is paired with state $s$ and $c(x)$ the number of times we see the word $x$ on the training set. Then, we set\n",
    "\n",
    "$$\n",
    "q(u|v, s) = \\frac{c(u, v, s)}{c(v, s)} \\;\\;\\;\\;\\;\\;\\;\\text{and} \\;\\;\\;\\;\\;\\;\\; e(x|s) = \\frac{c(s \\leadsto x)}{c(x)}\n",
    "$$\n",
    "\n",
    "### Decoding\n",
    "\n",
    "In order to tag the sequence we need to evaluate the function $f$, the problem is that if the sentece has length $n$ then there are $|\\mathcal{K}|^n$ different ways to tag it. This means that testing every single one is not a wise choice, alternatively though, one can use the Viterbi algorithm. \n",
    "\n",
    "First define $r(y_{-1}, y_0, y_1, \\dots, y_k)$ to be the truncated version of the function $p$ defined previously, where $y_{-1}=y_0=*$ a special symbol to denote invalid words. Moreover denote, $\\mathcal{K}_k=\\mathcal{K}, \\forall k=1, \\dots, n$ and $\\mathcal{K}_{-1}=\\mathcal{K}_0=\\{*\\}$. Thus, we can also set\n",
    "\n",
    "$$\n",
    "S(k, u, v) = \\{\\langle y_{-1}y_0y_1\\dots y_k \\rangle|y_{k-1}=u, y_{k}=v, y_i \\in \\mathcal{K}_i, \\forall i \\ne k-1, k\\}\n",
    "$$\n",
    "\n",
    "In other words, $S$ is the set of all sequences of length $k$ that ends with the bigram $(u, v)$. Finally, also set\n",
    "\n",
    "$$\n",
    "\\pi(k, u, v) = \\max \\limits_{\\langle y_{-1}y_0y_1\\dots y_k \\rangle \\in S(k, u, v)} r(y_{-1}, y_0, y_1, \\dots, y_k)\n",
    "$$\n",
    "\n",
    "Now, we just need to realize that the following recursion hold true:\n",
    "$$\n",
    "\\pi(k, u, v) = \\max \\limits_{w \\in \\mathcal{K}_{k-2}} \\pi(k-1, w, v)\\times q(v|w,u) \\times e(x_k|v), \\;\\text{where}\\; \\pi(0, *, *) = 1\n",
    "$$\n",
    "\n",
    "And, after denoting $y_{n+1} = STOP$ another special symbol to represent the end of the sentence, the decoding can be done as:\n",
    "$$\n",
    "\\max\\limits_{y_1, y_2, \\dots, y_{n+1}} p(x_1, x_2, \\dots, x_n) = \\max\\limits_{u \\in \\mathcal{K}_{n-1}, v \\in \\mathcal{K}_n} \\pi(n, u, v) \\times q(STOP|u, v)\n",
    "$$\n",
    "\n",
    "If one adds a pointer to track the indexes of the sequence of states that maximizes the probability $p$, we can easily recover the label in $\\mathcal{O}(n|\\mathcal{K}|^3)$.\n",
    "\n",
    "### Drawbacks\n",
    "\n",
    "The generative models do not handle unseen words, then it is necessary to map low frequency words to pseudo-words.\n",
    "\n",
    "## References\n",
    "\n",
    "All the ideas presented here are discussed in details in this link: http://www.cs.columbia.edu/~mcollins/hmms-spring2013.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import time\n",
    "from pprint import pprint\n",
    "from functools import reduce\n",
    "from operator import iconcat\n",
    "from nltk.tag.hmm import HiddenMarkovModelTrainer\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We will use the MIT movie corpus as a toy dataset to perform named entity recognition (NER), the data is available here: https://groups.csail.mit.edu/sls/downloads/movie/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(fname:str)->list:\n",
    "    with open(fname, \"r\") as f:\n",
    "        data = f.read()\n",
    "    data = data.split(\"\\n\\n\")\n",
    "    data = list(map(lambda x:x.split(\"\\n\"), data))\n",
    "    data = list(map(lambda x:[tuple(s.split(\"\\t\"))[::-1] for s in x], data))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = process_file(\"./datasets/engtrain.bio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('what', 'O'),\n",
       "  ('movies', 'O'),\n",
       "  ('star', 'O'),\n",
       "  ('bruce', 'B-ACTOR'),\n",
       "  ('willis', 'I-ACTOR')],\n",
       " [('show', 'O'),\n",
       "  ('me', 'O'),\n",
       "  ('films', 'O'),\n",
       "  ('with', 'O'),\n",
       "  ('drew', 'B-ACTOR'),\n",
       "  ('barrymore', 'I-ACTOR'),\n",
       "  ('from', 'O'),\n",
       "  ('the', 'O'),\n",
       "  ('1980s', 'B-YEAR')],\n",
       " [('what', 'O'),\n",
       "  ('movies', 'O'),\n",
       "  ('starred', 'O'),\n",
       "  ('both', 'O'),\n",
       "  ('al', 'B-ACTOR'),\n",
       "  ('pacino', 'I-ACTOR'),\n",
       "  ('and', 'O'),\n",
       "  ('robert', 'B-ACTOR'),\n",
       "  ('deniro', 'I-ACTOR')]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format nltk uses to represent tagged sentences is quite simple, the training set is a list of sentences. Each sentence is a list of tuples, the first element is a word and the second is its corresponding tag. Also note that, the tag O represents that the corresponding word is not an entity, and B-SOMETHING represents the entity beginning and I-SOMETHING represents both the intermediate and last words of the entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('',)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.pop() # Last sentence is empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are 9775 sentences in the training set'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"There are {len(train)} sentences in the training set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_list(data:list)->list:\n",
    "    return reduce(iconcat, data, [])\n",
    "\n",
    "def split_words_n_tags(data:list)->tuple:\n",
    "    words, tags = map(list, zip(*data))\n",
    "    return words, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pairs = to_list(train)\n",
    "all_words, all_tags = split_words_n_tags(all_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are 6710 unique words in the training set'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"There are {len(set(all_words))} unique words in the training set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are 25 unique tags in the training set'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"There are {len(set(all_tags))} unique tags in the training set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('O', 61008),\n",
       " ('B-GENRE', 4354),\n",
       " ('I-TITLE', 3495),\n",
       " ('I-ACTOR', 3474),\n",
       " ('B-ACTOR', 3220),\n",
       " ('B-YEAR', 2858),\n",
       " ('I-YEAR', 2456),\n",
       " ('B-TITLE', 2376),\n",
       " ('B-RATING', 2007),\n",
       " ('B-PLOT', 1927),\n",
       " ('B-RATINGS_AVERAGE', 1869),\n",
       " ('I-DIRECTOR', 1850),\n",
       " ('B-DIRECTOR', 1720),\n",
       " ('I-PLOT', 1687),\n",
       " ('I-RATINGS_AVERAGE', 1673),\n",
       " ('I-RATING', 840),\n",
       " ('I-GENRE', 786),\n",
       " ('I-SONG', 446),\n",
       " ('B-CHARACTER', 385),\n",
       " ('I-CHARACTER', 342),\n",
       " ('B-SONG', 245),\n",
       " ('B-REVIEW', 221),\n",
       " ('I-REVIEW', 132),\n",
       " ('B-TRAILER', 113),\n",
       " ('I-TRAILER', 7)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist = nltk.probability.FreqDist(all_tags)\n",
    "hist.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class $\\textit{nltk.probability.FreqDist}$ builds a histogram that tells us how many times each tag appeared on the training set. We can already expect that the tags with low frequency will be the ones the model will have more trouble during testing time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "It is quite easy to train an HMM using the nltk API, the lack of hyperparameters contribuites a lot to it. In our case, the method we are about to call is the $\\textit{train_supervised}$, given the way we processed the sentences is enough to feed the variable $\\textit{train}$ directly to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HMM took 0.13901 seconds to train'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tic = time.time()\n",
    "hmm = HiddenMarkovModelTrainer().train_supervised(train)\n",
    "toc = time.time()\n",
    "f\"HMM took {(toc-tic):.5f} seconds to train\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick a sentence in the test set to see what we can get from the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('',)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = process_file(\"./datasets/engtest.bio\")\n",
    "test.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['list', 'the', 'five', 'star', 'rated', 'movies', 'starring', 'mel', 'gibson']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy = [w for w,t in test[2]]\n",
    "toy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2529096 , 0.59349388, 0.49746641, 1.0173373 , 1.53786785,\n",
       "       0.94886663, 0.5767745 , 0.57020132, 1.25574454])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm.point_entropy(toy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.048307571135144"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm.entropy(toy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\textit{point_entropy}$ method gives the amount of uncertainty the model is about the tags assign to each word in the sentence. We can also recover the entropy for the whole sentence using the $\\textit{entropy}$ method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's tag this sequence and compare it with the actual states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('list', 'O'),\n",
       " ('the', 'O'),\n",
       " ('five', 'B-RATINGS_AVERAGE'),\n",
       " ('star', 'I-RATINGS_AVERAGE'),\n",
       " ('rated', 'I-RATINGS_AVERAGE'),\n",
       " ('movies', 'O'),\n",
       " ('starring', 'O'),\n",
       " ('mel', 'B-ACTOR'),\n",
       " ('gibson', 'I-ACTOR')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm.tag(toy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('list', 'O'),\n",
       " ('the', 'O'),\n",
       " ('five', 'B-RATINGS_AVERAGE'),\n",
       " ('star', 'I-RATINGS_AVERAGE'),\n",
       " ('rated', 'O'),\n",
       " ('movies', 'O'),\n",
       " ('starring', 'O'),\n",
       " ('mel', 'B-ACTOR'),\n",
       " ('gibson', 'I-ACTOR')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall note that the word \"rated\" has the highest entropy, thus, is the one in the sentence the model is more uncertain about. It is also the only label the model made a mistake, but it is important to keep in mind that high confidence does really mean the prediction is right, as an example take the case down below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['are', 'there', 'any', 'good', 'romantic', 'comedies', 'out', 'right', 'now']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy = [w for w,t in test[0]]\n",
    "toy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm.point_entropy(toy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('are', 'O'),\n",
       " ('there', 'O'),\n",
       " ('any', 'O'),\n",
       " ('good', 'O'),\n",
       " ('romantic', 'B-GENRE'),\n",
       " ('comedies', 'I-GENRE'),\n",
       " ('out', 'O'),\n",
       " ('right', 'O'),\n",
       " ('now', 'O')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm.tag(toy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('are', 'O'),\n",
       " ('there', 'O'),\n",
       " ('any', 'O'),\n",
       " ('good', 'O'),\n",
       " ('romantic', 'B-GENRE'),\n",
       " ('comedies', 'I-GENRE'),\n",
       " ('out', 'O'),\n",
       " ('right', 'B-YEAR'),\n",
       " ('now', 'I-YEAR')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is high confident about all its predictions but it got the last two words wrongly, maybe due to the fact that the word \"out\" is often followed by the year the movie will be released."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's test our model on all sentences in the testing set to see how it performed. Firstly, we will unlabel the testing set to simulate when real predictions are desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrive_sents(data:list)->list:\n",
    "    return list(map(lambda x:[w for w,t in x], data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, labels = split_words_n_tags(to_list(test))\n",
    "unlabeled_sents = retrive_sents(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['are',\n",
       "  'there',\n",
       "  'any',\n",
       "  'good',\n",
       "  'romantic',\n",
       "  'comedies',\n",
       "  'out',\n",
       "  'right',\n",
       "  'now'],\n",
       " ['show', 'me', 'a', 'movie', 'about', 'cars', 'that', 'talk'],\n",
       " ['list',\n",
       "  'the',\n",
       "  'five',\n",
       "  'star',\n",
       "  'rated',\n",
       "  'movies',\n",
       "  'starring',\n",
       "  'mel',\n",
       "  'gibson']]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_sents[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By calling the method $\\textit{tag_sents}$ we can tag all sentences at once using the HMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HMM took 2.54075 seconds to tag all sequences in the testing set'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tic = time.time()\n",
    "preds = hmm.tag_sents(unlabeled_sents)\n",
    "toc = time.time()\n",
    "f\"HMM took {(toc-tic):.5f} seconds to tag all sequences in the testing set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'O', 'O', 'O', 'B-GENRE', 'I-GENRE', 'O', 'O', 'O', 'O']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, preds = split_words_n_tags(to_list(preds))\n",
    "preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'O', 'O', 'O', 'B-GENRE', 'I-GENRE', 'O', 'B-YEAR', 'I-YEAR', 'O']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a list with all predicted tags regardless the sentence, since we are not interested in sentence-wise performance. The $\\textit{labels}$ variable is a list paired with the $\\textit{preds}$, so we can call the $\\textit{classification_report}$ function from the $\\textit{sklearn}$ API to measure the accuracy of the HMM's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('                   precision    recall  f1-score   support\\n'\n",
      " '\\n'\n",
      " '          B-ACTOR       0.87      0.82      0.84       812\\n'\n",
      " '      B-CHARACTER       0.40      0.37      0.38        90\\n'\n",
      " '       B-DIRECTOR       0.80      0.59      0.68       456\\n'\n",
      " '          B-GENRE       0.92      0.91      0.92      1117\\n'\n",
      " '           B-PLOT       0.58      0.47      0.52       491\\n'\n",
      " '         B-RATING       0.97      0.90      0.94       500\\n'\n",
      " 'B-RATINGS_AVERAGE       0.88      0.73      0.80       451\\n'\n",
      " '         B-REVIEW       0.17      0.20      0.18        56\\n'\n",
      " '           B-SONG       0.33      0.24      0.28        54\\n'\n",
      " '          B-TITLE       0.64      0.45      0.53       562\\n'\n",
      " '        B-TRAILER       0.84      0.87      0.85        30\\n'\n",
      " '           B-YEAR       0.93      0.82      0.87       720\\n'\n",
      " '          I-ACTOR       0.91      0.76      0.83       862\\n'\n",
      " '      I-CHARACTER       0.45      0.32      0.38        75\\n'\n",
      " '       I-DIRECTOR       0.86      0.53      0.65       496\\n'\n",
      " '          I-GENRE       0.89      0.67      0.76       222\\n'\n",
      " '           I-PLOT       0.52      0.28      0.37       496\\n'\n",
      " '         I-RATING       0.62      0.88      0.73       226\\n'\n",
      " 'I-RATINGS_AVERAGE       0.74      0.78      0.76       403\\n'\n",
      " '         I-REVIEW       0.21      0.24      0.23        45\\n'\n",
      " '           I-SONG       0.55      0.39      0.46       119\\n'\n",
      " '          I-TITLE       0.66      0.45      0.54       856\\n'\n",
      " '        I-TRAILER       0.00      0.00      0.00         8\\n'\n",
      " '           I-YEAR       0.97      0.82      0.89       610\\n'\n",
      " '                O       0.86      0.95      0.90     14929\\n'\n",
      " '\\n'\n",
      " '         accuracy                           0.84     24686\\n'\n",
      " '        macro avg       0.66      0.58      0.61     24686\\n'\n",
      " '     weighted avg       0.83      0.84      0.83     24686\\n')\n"
     ]
    }
   ],
   "source": [
    "pprint(metrics.classification_report(labels, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous table we can see the precision, recall and f1-score for each tag. The B-CHARACTER, B-REVIEW, B-SONG, I-CHARACTER, I-REVIEW and I-TRAILER were the most difficult tags to model by the HMM, oddly enough, they are also the ones that have the smallest support. Further analysis is necessary to tell if this model is indeed robust to make actual predictions for this dataset, but since this is an introductory example we will stop here :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
